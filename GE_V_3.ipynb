{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Round=145\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "import os.path\n",
    "import numerapi\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "import random, timeit, string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Samit.singh\\\\Jupyter_Notebooks\\\\Genetic_Ensemble\\\\GE_WIP'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no new round within the last 24 hours\n"
     ]
    }
   ],
   "source": [
    "napi = numerapi.NumerAPI(verbosity=\"info\")\n",
    "if napi.check_new_round():\n",
    "    print(\"new round has started within the last 24hours!\")\n",
    "else:\n",
    "    print(\"no new round within the last 24 hours\")\n",
    "\n",
    "def load_data(ro):\n",
    "    ros=str(ro)\n",
    "    if os.path.isfile('../datasets/'+ros+'-train.csv'):\n",
    "        return pd.read_csv('../datasets/'+ros+'-train.csv'),pd.read_csv('../datasets/'+ros+'-tour.csv')\n",
    "    else:\n",
    "        print ('Checking Round')\n",
    "        cc=napi.get_current_round()\n",
    "        if cc==ro:\n",
    "            print ('Downloading files for Round ',ro)\n",
    "            napi.download_current_dataset(unzip=True)\n",
    "        else:\n",
    "            print ('Older round or round not started')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train,tour=load_data(Round)\n",
    "# ship = tour[tour['data_type']=='validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10', 'feature11', 'feature12', 'feature13', 'feature14', 'feature15', 'feature16', 'feature17', 'feature18', 'feature19', 'feature20', 'feature21', 'feature22', 'feature23', 'feature24', 'feature25', 'feature26', 'feature27', 'feature28', 'feature29', 'feature30', 'feature31', 'feature32', 'feature33', 'feature34', 'feature35', 'feature36', 'feature37', 'feature38', 'feature39', 'feature40', 'feature41', 'feature42', 'feature43', 'feature44', 'feature45', 'feature46', 'feature47', 'feature48', 'feature49', 'feature50']\n",
      "['bernie', 'elizabeth', 'jordan', 'ken', 'charles', 'frank', 'hillary']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bernie'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=[f for f in list(train) if \"feature\" in f]\n",
    "print (features)\n",
    "targets=[f.split('_')[1] for f in list(train) if \"target\" in f]\n",
    "print (targets)\n",
    "target=targets[0]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(836656, 50)\n"
     ]
    }
   ],
   "source": [
    "all_data = np.concatenate([train[features].values, tour[features].values],axis=0)\n",
    "print (all_data.shape)\n",
    "def get_tsne_features(all_data):\n",
    "    pca = PCA(n_components=9)\n",
    "    data_pca = pca.fit_transform(all_data)\n",
    "    tsne = TSNE(n_jobs=8)\n",
    "    data_tsne =  TSNE(n_components=2).fit_transform(data_pca)\n",
    "    return data_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tsne = get_tsne_features(all_data)\n",
    "np.save(\"all_data_tsne\",data_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tsne = np.load(\"all_data_tsne.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TSNE features calculated to train and tour data\n",
    "train[\"TSNE_1\"] = data_tsne[:train.shape[0],0]\n",
    "train[\"TSNE_2\"] = data_tsne[:train.shape[0],1]\n",
    "tour[\"TSNE_1\"] = data_tsne[train.shape[0]:train.shape[0]+tour.shape[0],0]\n",
    "tour[\"TSNE_2\"] = data_tsne[train.shape[0]:train.shape[0]+tour.shape[0],1]\n",
    "ship = tour[tour['data_type']=='validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA_transformed(n):\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(train[features])\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rounder(x,level=3):\n",
    "    if x>=.7:\n",
    "        return .7\n",
    "    if x<.3:\n",
    "        return .3\n",
    "    mu=10**level\n",
    "    x=x*mu\n",
    "    if x>(mu/2):\n",
    "        return (int(x)+1)/float(mu)\n",
    "    return int(x)/float(mu)\n",
    "ron = np.vectorize(rounder)\n",
    "\n",
    "def loss(y,yp):\n",
    "    return metrics.log_loss(y,ron(yp))\n",
    "\n",
    "def exo(cut,pca = 24,onTrain=0):\n",
    "    pca = PCA_transformed(pca)\n",
    "    X=pca.transform(cut[features])\n",
    "    X = np.concatenate([X,cut[[\"TSNE_1\",\"TSNE_2\"]]],axis=1)     # adding TSNE features\n",
    "    if onTrain==2:\n",
    "        return X\n",
    "    y=cut['target_'+target]\n",
    "    if onTrain:\n",
    "        return X,y\n",
    "    yp=cut['pred']\n",
    "    return X,y,yp,loss(y,yp)\n",
    "\n",
    "# Caluclates consistency score and log loss\n",
    "def eraLoss(frame,pred,pca):\n",
    "    \n",
    "    eras=frame['era'].unique()\n",
    "    frame['pred']=pred\n",
    "    li=[]\n",
    "    for era in eras:\n",
    "        X,y,yp,l=exo(frame[frame['era']==era], pca=pca)\n",
    "        # X = np.concatenate(X,tsne_features)\n",
    "        li.append(l)\n",
    "    li=np.array(li)\n",
    "    X,y,yp,l=exo(frame,pca)\n",
    "    c=len(li[li<.693])*100/len(li)\n",
    "    print ('Loss:', l, '- at ',c,'%')\n",
    "    print ('-- -- '*8)\n",
    "    print()\n",
    "    return l, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rr(x,y):\n",
    "    if type(x)==type(1) and type(y)==type(1):\n",
    "        x=int(x)\n",
    "        y=int(y)\n",
    "        return random.randint(x,y+1)\n",
    "    return random.uniform(x,y)\n",
    "\n",
    "def rrr(a,b): #test b4 use\n",
    "    if type(a) == int and type(b) == int:\n",
    "        c= int((b-a)/4)\n",
    "    else:\n",
    "        c= (b-a)/4\n",
    "    if random.choice([0,1]):\n",
    "        return rr(a,a+c)                   # search in first  quadrant\n",
    "    return rr(b-c,b)                       # search in fourth quadrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def param_gen(emap):                       # emap is a dictionary which has range of values [a,b] for each parameter\n",
    "    m={}\n",
    "    for i in emap:\n",
    "        m[i]=rrr(emap[i][0],emap[i][1])    # for each paramter in the dict provide the upper and lower limit values i.e a and b\n",
    "    return m                               # return a dictionary with single value for each parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mutate(a,b):\n",
    "    ones=[a,b]\n",
    "    if type(a) == int and type(b) == int:\n",
    "        muls=[int((a+b)/2),int((2*a+b)/3),int((2*b+a)/3)]\n",
    "    else:\n",
    "        muls=[(a+b)/2,(2*a+b)/3,(2*b+a)/3]\n",
    "#     print (muls)\n",
    "#     print (ones+muls+muls)\n",
    "    return random.choice(ones+muls+muls)\n",
    "\n",
    "def params_child(p1,p2):\n",
    "#     print (p1,p2)\n",
    "    m={}\n",
    "    for i in p1:                         \n",
    "        if p1[i]==p2[i]:\n",
    "            m[i]=p1[i]                          # When parameters are same for both the parents keep them in child else mutate\n",
    "        else:                                   \n",
    "            m[i]=mutate(p1[i],p2[i])\n",
    "    if m==p1:\n",
    "        return 0\n",
    "    if m==p2:\n",
    "        return 0\n",
    "    return m\n",
    "def rands(n):\n",
    "    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModel(ty='xgb',params={}):\n",
    "    if ty=='xgb':\n",
    "        copy = dict(params)                # making a copy of dict, to delete key safely\n",
    "        del copy[\"pca\"]\n",
    "        copy['nthread']=-1\n",
    "        return XGBClassifier(**copy)\n",
    "    if ty=='cb':\n",
    "        copy = dict(params)\n",
    "        del copy[\"pca\"]\n",
    "        copy['thread_count']= 4\n",
    "        copy['verbose']= False\n",
    "        return CatBoostClassifier(**copy)\n",
    "    if ty=='lgbm':\n",
    "        copy = dict(params)\n",
    "        del copy[\"pca\"]\n",
    "        return LGBMClassifier(**copy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maps={\n",
    "    'xgb':{'params':{'max_depth':[1,15],'n_estimators':[10,2000],'learning_rate':[.01,.9],'gamma':[0,100], 'min_child_weight':[1,20],\n",
    "           'max_delta_step':[1,1000], 'subsample':[.05,1.0],'colsample_bytree':[.05,1.0], \n",
    "           'colsample_bylevel':[.05,1.0], 'reg_alpha':[0,1000], 'reg_lambda':[0,1000], 'pca': [5,35]},\n",
    "           'hookups':3,\n",
    "           'children':3,\n",
    "           'asuras':2,\n",
    "           'survivors':5\n",
    "          },\n",
    "    'lgbm':{'params':{'num_leaves':[2,80],'max_depth':[1,20],'n_estimators':[10,2000],'learning_rate':[.01,.9],\n",
    "           'subsample':[.05,1.0],'colsample_bytree':[.05,1.0],'reg_alpha':[0,1000],'reg_lambda':[0,1000],'pca': [5,35]},\n",
    "           'hookups':10,\n",
    "           'children':4,\n",
    "           'asuras':6,\n",
    "           'survivors':9\n",
    "          },\n",
    "#     'cb':{'params':{'depth':[1,15],\n",
    "#           'iterations':[250,750],\n",
    "#           'learning_rate':[0.03,0.3], \n",
    "#           'l2_leaf_reg':[3,100],\n",
    "#           'border_count':[32,200],'pca': [5,35]\n",
    "#           },\n",
    "#           'hookups':3,\n",
    "#            'children':4,\n",
    "#            'asuras':4,\n",
    "#           'survivors':5\n",
    "#          }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this populates the performance of all the classifiers in genome dataframe\n",
    "def simulate():\n",
    "    for i,j in genome.iterrows():\n",
    "        if not j['ran']:\n",
    "            clock1=timeit.timeit()\n",
    "            print (j['name'])\n",
    "            clf=getModel(j['type'],j['params'])\n",
    "            X,y = exo(train,j['params'][\"pca\"],1)\n",
    "            Xt,yt=exo(ship,j['params'][\"pca\"],1)\n",
    "            clf.fit(X,y)\n",
    "            yp=clf.predict_proba(Xt)[:,1]\n",
    "            l,c=eraLoss(ship,yp,j['params'][\"pca\"])\n",
    "            clock2=timeit.timeit()\n",
    "            genome.loc[i,'log_loss']=l\n",
    "            genome.loc[i,'consistency']=c\n",
    "            genome.loc[i,'delay']=round((clock2-clock1)*1000,2)\n",
    "            genome.loc[i,'ran']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genesis():\n",
    "    for m in maps:\n",
    "        for i in range(maps[m]['asuras']):\n",
    "            genome.loc[len(genome)]=[m+'-gen0-orig-'+rands(4),m,param_gen(maps[m]['params']),.8,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reproduce(gen):\n",
    "    for m in maps:\n",
    "        for h in range(maps[m]['hookups']):\n",
    "            parents = genome[genome['type']==m].sample(n=2)\n",
    "            for c in range(maps[m]['children']):\n",
    "                child=params_child(parents.iloc[0]['params'],parents.iloc[1]['params'])\n",
    "                if child:\n",
    "                    genome.loc[len(genome)]=[m+'-gen'+str(gen)+'-h'+str(h)+'-c'+str(c)+'-'+rands(4),m,child,0.8,0,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def war(genome):\n",
    "    count=0\n",
    "    survivors=[]\n",
    "    for m in maps:\n",
    "        if count:\n",
    "            sids=genome[genome['type']==m].sort_values(['consistency','log_loss'], ascending=[False,True]).head(maps[m]['survivors']).index\n",
    "            survivors=survivors.append(sids)\n",
    "        else:\n",
    "            survivors=genome[genome['type']==m].sort_values(['consistency','log_loss'], ascending=[False,True]).head(maps[m]['survivors']).index\n",
    "        count=count+1\n",
    "    return genome.iloc[survivors].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genome=pd.DataFrame(columns=['name','type','params','log_loss','consistency','delay','ran'])\n",
    "\n",
    "# genome=pd.read_csv('../genome.csv')\n",
    "# genome['params']=genome['params'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "genesis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submission_func(prediction_probabilities, path, target):\n",
    "    results_df = pd.DataFrame(data=prediction_probabilities,columns=['probability_'+target])\n",
    "    joined = pd.DataFrame(tour['id']).join(results_df)\n",
    "    assert(len(joined) == len(tour))\n",
    "    joined.to_csv(path, index=False)\n",
    "    \n",
    "    # Adding public and private keys\n",
    "    a='6X4PZS5QYKRW5RIN6LK4PY3KGAIQBZJPEW5CPEXNAM6IRBDTDLG4QV3NFWMMJQZI'\n",
    "    b='WWYHT5NBSLOPKL6VLPNFRZMRG5MHDKPI'\n",
    "    napi = numerapi.NumerAPI(b, a)\n",
    "    \n",
    "    tim={'bernie':1, 'charles':5, 'elizabeth':2, 'jordan':3, 'ken':4, 'frank':6, 'hillary':7}\n",
    "    \n",
    "    try:\n",
    "        submission_id = napi.upload_predictions(path,tournament=tim[target])\n",
    "        print (\"SUBMISSION UPLOADED for {}\".format(target))\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        print (\"\\n Error Uploading submission for {} \\n\".format(target))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ken\n",
      "xgb-gen0-orig-YE94\n",
      "Loss: 0.6931471805599453 - at  0.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen0-orig-16AR\n",
      "Loss: 0.6921762348117177 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-WXV5\n",
      "Loss: 0.6923210052634861 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-THVX\n",
      "Loss: 0.6924393525199921 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-SC4K\n",
      "Loss: 0.6926043643034558 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-QRAJ\n",
      "Loss: 0.7039106012970979 - at  0.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-KB3M\n",
      "Loss: 0.6922488838812338 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-XM25\n",
      "Loss: 0.6927027998942173 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen2-h0-c0-9KQI\n",
      "Loss: 0.6927405520958283 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen2-h0-c1-Z9BV\n",
      "Loss: 0.6926274264682146 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen2-h0-c2-CROG\n",
      "Loss: 0.6931471805599453 - at  0.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen2-h1-c0-2X31\n",
      "Loss: 0.6925307761797536 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen2-h1-c1-9412\n",
      "Loss: 0.6924572483450971 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen2-h1-c2-YNUE\n",
      "Loss: 0.6924313954910365 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen2-h2-c0-28V2\n",
      "Loss: 0.6927148377663438 - at  50.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen2-h2-c1-M97Y\n",
      "Loss: 0.6928014260635821 - at  58.333333333333336 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen2-h2-c2-HWO0\n",
      "Loss: 0.6927737685177443 - at  58.333333333333336 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h0-c0-TLZX\n",
      "Loss: 0.6925338570060693 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h0-c1-HNDM\n",
      "Loss: 0.692627214422521 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h0-c2-EFMP\n",
      "Loss: 0.6925732665865878 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h0-c3-YG2S\n",
      "Loss: 0.6926119503996712 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h1-c0-Q5V4\n",
      "Loss: 0.692596234448986 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h1-c1-D4QP\n",
      "Loss: 0.69255471415358 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h1-c2-ZYR1\n",
      "Loss: 0.6925840216306299 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h1-c3-T7QO\n",
      "Loss: 0.6925907117142025 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h2-c0-81QS\n",
      "Loss: 0.692562524100757 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h2-c1-0UBA\n",
      "Loss: 0.6925400890116857 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h2-c2-1GJT\n",
      "Loss: 0.6925689067638944 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h2-c3-RZTJ\n",
      "Loss: 0.692591326302502 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h3-c0-JKRF\n",
      "Loss: 0.6925610443340003 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h3-c1-6339\n",
      "Loss: 0.6925587590542852 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h3-c2-E5CV\n",
      "Loss: 0.6925582836203799 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h3-c3-AKZX\n",
      "Loss: 0.6925510437751595 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h4-c0-2W5M\n",
      "Loss: 0.6925985673213205 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h4-c1-E088\n",
      "Loss: 0.6925966869113235 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h4-c2-O172\n",
      "Loss: 0.6925950591728341 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h4-c3-IMT4\n",
      "Loss: 0.6925872170409005 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h5-c0-391P\n",
      "Loss: 0.6924641297308783 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h5-c1-58CD\n",
      "Loss: 0.6924790265752739 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h5-c2-AEB4\n",
      "Loss: 0.7003558041543169 - at  0.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h5-c3-IK2B\n",
      "Loss: 0.6924627178446608 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h6-c0-KJ5Z\n",
      "Loss: 0.6925709709324783 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h6-c1-HA49\n",
      "Loss: 0.692602397329003 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h6-c2-3HXQ\n",
      "Loss: 0.6925677792229095 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h6-c3-QWJX\n",
      "Loss: 0.6925655286591925 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h7-c0-HS9A\n",
      "Loss: 0.6925997178693747 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h7-c1-2SLP\n",
      "Loss: 0.6925651308047833 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h7-c2-K110\n",
      "Loss: 0.692584335645477 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h7-c3-BFGO\n",
      "Loss: 0.6925997178693747 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h8-c0-JGJG\n",
      "Loss: 0.6926074532455156 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h8-c1-64WO\n",
      "Loss: 0.6926316006491483 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h8-c2-Z4F0\n",
      "Loss: 0.6926251923888693 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h8-c3-06AK\n",
      "Loss: 0.6925544812711348 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h9-c0-8GJE\n",
      "Loss: 0.6926241218416019 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h9-c1-NLD5\n",
      "Loss: 0.6925546021609463 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h9-c2-YGLS\n",
      "Loss: 0.6926022731906097 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen2-h9-c3-4NAF\n",
      "Loss: 0.6926033347868977 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen0-orig-3IRN\n",
      "Loss: 0.6931478967736309 - at  0.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen0-orig-7FRD\n",
      "Loss: 0.6928288299012302 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-8UPH\n",
      "Loss: 0.6922258345181134 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-PEUR\n",
      "Loss: 0.6930870441438574 - at  50.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-LJSY\n",
      "Loss: 0.6926161269544658 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-EIXO\n",
      "Loss: 0.6926577642710757 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-HE8E\n",
      "Loss: 0.6925283227060812 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-V6DA\n",
      "Loss: 0.69221534327023 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen3-h0-c0-56Z7\n",
      "Loss: 0.6924880228049335 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen3-h0-c1-DOPM\n",
      "Loss: 0.6926981902224479 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen3-h0-c2-8VRO\n",
      "Loss: 0.6925513793889033 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen3-h1-c0-S3K2\n",
      "Loss: 0.6923959952049127 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen3-h1-c1-Q42D\n",
      "Loss: 0.6924388729491854 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen3-h1-c2-IK9T\n",
      "Loss: 0.6923564462634215 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen3-h2-c0-5S61\n",
      "Loss: 0.6925737218568349 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen3-h2-c1-4JLT\n",
      "Loss: 0.6924226934157236 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen3-h2-c2-DF8P\n",
      "Loss: 0.6923802737929271 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h0-c0-GAFQ\n",
      "Loss: 0.6924998261157422 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h0-c1-KOEG\n",
      "Loss: 0.6924478214146809 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h0-c2-0CBX\n",
      "Loss: 0.6925588297407262 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h0-c3-4XK6\n",
      "Loss: 0.6925908370216497 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h1-c0-AGYI\n",
      "Loss: 0.6924130793970324 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h1-c1-V5TP\n",
      "Loss: 0.6924886800343403 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h1-c2-X4VQ\n",
      "Loss: 0.6924844337695611 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h1-c3-8SZY\n",
      "Loss: 0.6924108107174449 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h2-c0-4Q0K\n",
      "Loss: 0.6931793518303403 - at  41.666666666666664 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h2-c1-KNDJ\n",
      "Loss: 0.6922627876034367 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h2-c2-IJ35\n",
      "Loss: 0.6923870717378171 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h2-c3-9STQ\n",
      "Loss: 0.6923910450648972 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h3-c0-GE1R\n",
      "Loss: 0.6923934228538693 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h3-c1-3YWI\n",
      "Loss: 0.6923156885378985 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h3-c2-X7HM\n",
      "Loss: 0.6923883971168142 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h3-c3-GS93\n",
      "Loss: 0.6924067042915663 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h4-c0-66LX\n",
      "Loss: 0.6924815919262483 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h4-c1-VVCG\n",
      "Loss: 0.6924837847265042 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h4-c2-OCQL\n",
      "Loss: 0.6924844133866485 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h4-c3-V0V8\n",
      "Loss: 0.6924977970460338 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h5-c0-WIJQ\n",
      "Loss: 0.6924118833159159 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h5-c1-TQ89\n",
      "Loss: 0.6924450523769159 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h5-c2-IX5K\n",
      "Loss: 0.692415745395638 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h5-c3-A37Y\n",
      "Loss: 0.6924243009812651 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h6-c0-153H\n",
      "Loss: 0.6925080345618321 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h6-c1-DILY\n",
      "Loss: 0.6924885545447697 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h6-c2-5JYQ\n",
      "Loss: 0.6925018450388107 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h6-c3-JLSB\n",
      "Loss: 0.692517489455911 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h7-c0-PRKZ\n",
      "Loss: 0.6925282781393663 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h7-c1-L2T9\n",
      "Loss: 0.6925827345716102 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h7-c2-VYKT\n",
      "Loss: 0.6926159635301115 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h7-c3-LU5J\n",
      "Loss: 0.6925899714482318 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h8-c0-9UKN\n",
      "Loss: 0.6924942184096268 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h8-c1-D6AQ\n",
      "Loss: 0.6923803104121249 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h8-c2-4A2I\n",
      "Loss: 0.6924176610889133 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h8-c3-9UFU\n",
      "Loss: 0.6923800140298519 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h9-c0-CR6T\n",
      "Loss: 0.6924835357893004 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h9-c1-83CX\n",
      "Loss: 0.6924432433634086 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h9-c2-IFR9\n",
      "Loss: 0.6924914319440935 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen3-h9-c3-8HFX\n",
      "Loss: 0.6925345818408509 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen0-orig-VDVT\n",
      "Loss: 0.6926251556599858 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen0-orig-0TBQ\n",
      "Loss: 0.6925034941418489 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-IPSX\n",
      "Loss: 0.6924800621096624 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-QSXG\n",
      "Loss: 0.6925824511182492 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-7QX9\n",
      "Loss: 0.692509994706179 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-RKXR\n",
      "Loss: 0.6925803970748304 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-BXA7\n",
      "Loss: 0.6924924912939339 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-6LFN\n",
      "Loss: 0.6926104026571693 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen4-h0-c0-PUN6\n",
      "Loss: 0.6923559558286874 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen4-h0-c1-OMLR\n",
      "Loss: 0.6924918155774699 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen4-h0-c2-73A9\n",
      "Loss: 0.692380494685744 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen4-h1-c0-4KN4\n",
      "Loss: 0.6924226119544673 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen4-h1-c1-U6BD\n",
      "Loss: 0.6925316433223939 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen4-h1-c2-W79X\n",
      "Loss: 0.6924264203240781 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen4-h2-c0-EI8W\n",
      "Loss: 0.6924687505399629 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen4-h2-c1-BYV3\n",
      "Loss: 0.6926048247412181 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen4-h2-c2-TRJC\n",
      "Loss: 0.6923145309767194 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h0-c0-OPZP\n",
      "Loss: 0.6922583047798343 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h0-c1-1V9M\n",
      "Loss: 0.692245767469172 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h0-c2-8ETN\n",
      "Loss: 0.6923046605067703 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h0-c3-C22D\n",
      "Loss: 0.6922747939661557 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h1-c0-0WFB\n",
      "Loss: 0.6922383508060674 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h1-c1-JRUR\n",
      "Loss: 0.6922739397800182 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h1-c2-6E17\n",
      "Loss: 0.6922714868625363 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h1-c3-66RK\n",
      "Loss: 0.6922699630830292 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h2-c0-VEI9\n",
      "Loss: 0.6919891405902885 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h2-c1-HIXP\n",
      "Loss: 0.6921655604087167 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h2-c2-GCB3\n",
      "Loss: 0.6921302192941945 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h2-c3-QDB7\n",
      "Loss: 0.6921131190902216 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h3-c0-KR7G\n",
      "Loss: 0.6923142978592745 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h3-c1-NJ0M\n",
      "Loss: 0.6923550089703671 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h3-c2-ESUY\n",
      "Loss: 0.6923666849042239 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h3-c3-S3MM\n",
      "Loss: 0.6923736992041203 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h4-c0-C122\n",
      "Loss: 0.6920461494552782 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h4-c1-JDSH\n",
      "Loss: 0.6919623122761089 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h4-c2-07E3\n",
      "Loss: 0.6922633699447063 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h4-c3-C0DN\n",
      "Loss: 0.6921781054745987 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h5-c0-2GG1\n",
      "Loss: 0.69223146575012 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h5-c1-TBO1\n",
      "Loss: 0.6923657142781054 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h5-c2-UKOH\n",
      "Loss: 0.6921867006156993 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h5-c3-7J0A\n",
      "Loss: 0.6923115592837726 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h6-c0-206S\n",
      "Loss: 0.6923694019745729 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h6-c1-ZS7I\n",
      "Loss: 0.6923003510294717 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h6-c2-35Q9\n",
      "Loss: 0.6945535942811465 - at  8.333333333333334 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h6-c3-EHBT\n",
      "Loss: 0.6922436022084408 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h7-c0-TDEM\n",
      "Loss: 0.6923297945757269 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h7-c1-YKBD\n",
      "Loss: 0.6922928195370324 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h7-c2-2CGL\n",
      "Loss: 0.6923094398671478 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h7-c3-LFFJ\n",
      "Loss: 0.6923047057117521 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h8-c0-S0OT\n",
      "Loss: 0.692381533592638 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h8-c1-P3IW\n",
      "Loss: 0.6923989620553233 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h8-c2-QX3S\n",
      "Loss: 0.692399664116411 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h8-c3-AWJJ\n",
      "Loss: 0.6923725760467992 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h9-c0-ZH1M\n",
      "Loss: 0.6922846933770531 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h9-c1-6JZ1\n",
      "Loss: 0.6922796551099555 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h9-c2-T98B\n",
      "Loss: 0.6923061282339694 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen4-h9-c3-07NA\n",
      "Loss: 0.6923050008439868 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen0-orig-H1LN\n",
      "Loss: 0.6931471805599453 - at  0.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "xgb-gen0-orig-0F33\n",
      "Loss: 0.6931471805599453 - at  0.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-19WS\n",
      "Loss: 0.6925497518265185 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-A7Y5\n",
      "Loss: 0.6924718492599904 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-3PWK\n",
      "Loss: 0.6926401192523883 - at  66.66666666666667 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-9RHH\n",
      "Loss: 0.692159189186321 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-V2VD\n",
      "Loss: 0.6921387994460229 - at  83.33333333333333 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "lgbm-gen0-orig-DBTU\n",
      "Loss: 0.6925651767893506 - at  75.0 %\n",
      "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "TRAIN: [     0      1      2 ... 502729 502730 502731] TEST: [  2425   2426   2427 ... 484902 484903 484904]\n",
      "TRAIN: [  2425   2426   2427 ... 484902 484903 484904] TEST: [     0      1      2 ... 502729 502730 502731]\n",
      "TRAIN: [     0      1      2 ... 502729 502730 502731] TEST: [  7255   7256   7257 ... 480506 480507 480508]\n",
      "TRAIN: [     0      1      2 ... 502729 502730 502731] TEST: [  4815   4816   4817 ... 453501 453502 453503]\n",
      "TRAIN: [     0      1      2 ... 502729 502730 502731] TEST: [  9818   9819   9820 ... 462594 462595 462596]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-07 17:11:59,397 INFO numerapi.numerapi: uploading predictions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBMISSION UPLOADED for ken\n"
     ]
    }
   ],
   "source": [
    "targets = [\"ken\"]\n",
    "for x in targets:\n",
    "    target=x\n",
    "    print (target)\n",
    "    \n",
    "    for generation in range(2,5):\n",
    "        reproduce(generation)\n",
    "        genesis()\n",
    "        simulate()\n",
    "        genome=war(genome)\n",
    "        genome.to_csv('../genome.csv',index=False)\n",
    "        genome.to_csv('../genome-gen'+str(generation)+'.csv',index=False)\n",
    "\n",
    "    # Stacking by kfold split\n",
    "\n",
    "\n",
    "    train_copy = train.copy()\n",
    "    tour_copy = tour.copy()\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import GroupKFold\n",
    "\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    X,Y = exo (train,1,onTrain=1)   # We just need index so PCA -1 does not matter here             \n",
    "    kfold_split = gkf.split(X, Y, groups=train.era)\n",
    "\n",
    "    for train_index, test_index in kfold_split:\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "        for i,j in genome.iterrows():\n",
    "            clf=getModel(j['type'],j['params'])\n",
    "            X,Y = exo(train,j['params'][\"pca\"],1)               # get X (train) as per n_components in PCA\n",
    "            Xt,yt=exo(tour,j['params'][\"pca\"],1)                # get X (tour) as per n_components in PCA\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = Y[train_index], Y[test_index]\n",
    "            clf.fit(X_train,y_train)\n",
    "            train_copy.loc[test_index,\"feature_{}_{}\".format(j['type'],i)]=clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "            clf.fit(X,Y)\n",
    "            tour_copy[\"feature_{}_{}\".format(j['type'],i)]=clf.predict_proba(Xt)[:,1]  \n",
    "\n",
    "\n",
    "    # Training a stacker\n",
    "    features_new=[f for f in list(train_copy) if \"feature_\" in f]\n",
    "    X_reg = train_copy[features_new]\n",
    "    y_reg = train_copy['target_'+target]\n",
    "\n",
    "    # Can add a different stacker here probably neural network!\n",
    "    stacker = LogisticRegression(max_iter=250).fit(X_reg,y_reg)\n",
    "    final_preds = stacker.predict_proba(tour_copy[features_new])[:,1]\n",
    "\n",
    "      \n",
    "    result_path ='../submissions/'+target+\"meta_model_stacked_submission.csv\"\n",
    "    \n",
    "    submission_func(final_preds,result_path,target)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
