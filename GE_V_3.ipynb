{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Round=145\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "import os.path\n",
    "import numerapi\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "import random, timeit, string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Samit.singh\\\\Jupyter_Notebooks\\\\Genetic_Ensemble\\\\GE_WIP'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no new round within the last 24 hours\n"
     ]
    }
   ],
   "source": [
    "napi = numerapi.NumerAPI(verbosity=\"info\")\n",
    "if napi.check_new_round():\n",
    "    print(\"new round has started within the last 24hours!\")\n",
    "else:\n",
    "    print(\"no new round within the last 24 hours\")\n",
    "\n",
    "def load_data(ro):\n",
    "    ros=str(ro)\n",
    "    if os.path.isfile('../datasets/'+ros+'-train.csv'):\n",
    "        return pd.read_csv('../datasets/'+ros+'-train.csv'),pd.read_csv('../datasets/'+ros+'-tour.csv')\n",
    "    else:\n",
    "        print ('Checking Round')\n",
    "        cc=napi.get_current_round()\n",
    "        if cc==ro:\n",
    "            print ('Downloading files for Round ',ro)\n",
    "            napi.download_current_dataset(unzip=True)\n",
    "        else:\n",
    "            print ('Older round or round not started')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train,tour=load_data(Round)\n",
    "ship = tour[tour['data_type']=='validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train.ix[:,-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10', 'feature11', 'feature12', 'feature13', 'feature14', 'feature15', 'feature16', 'feature17', 'feature18', 'feature19', 'feature20', 'feature21', 'feature22', 'feature23', 'feature24', 'feature25', 'feature26', 'feature27', 'feature28', 'feature29', 'feature30', 'feature31', 'feature32', 'feature33', 'feature34', 'feature35', 'feature36', 'feature37', 'feature38', 'feature39', 'feature40', 'feature41', 'feature42', 'feature43', 'feature44', 'feature45', 'feature46', 'feature47', 'feature48', 'feature49', 'feature50']\n",
      "['bernie', 'elizabeth', 'jordan', 'ken', 'charles', 'frank', 'hillary']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bernie'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=[f for f in list(train) if \"feature\" in f]\n",
    "print (features)\n",
    "targets=[f.split('_')[1] for f in list(train) if \"target\" in f]\n",
    "print (targets)\n",
    "target=targets[0]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502732, 9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = np.concatenate(train[features].values, tour[features].values,axis=0)\n",
    "\n",
    "def get_tsne_features(all_data):\n",
    "    pca = PCA(n_components=9)\n",
    "    data_pca = pca.fit_transform(all_data)\n",
    "    tsne = TSNE(n_jobs=8)\n",
    "    data_tsne =  TSNE(n_components=2).fit_transform(data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TSNE features calculated to train and tour data\n",
    "train[\"TSNE_1\"] = data_tsne[:train.shape[0],0]\n",
    "train[\"TSNE_2\"] = data_tsne[:train.shape[0],1]\n",
    "tour[\"TSNE_1\"] = data_tsne[train.shape[0]:train.shape[0]+tour.shape[0],0]\n",
    "tour[\"TSNE_2\"] = data_tsne[train.shape[0]:train.shape[0]+tour.shape[0],1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_jobs=8)\n",
    "X_embedded = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2).fit_transform(train[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.36552894, -1.786815  ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embedded[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA_transformed(n):\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(train[features])\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rounder(x,level=3):\n",
    "    if x>=.7:\n",
    "        return .7\n",
    "    if x<.3:\n",
    "        return .3\n",
    "    mu=10**level\n",
    "    x=x*mu\n",
    "    if x>(mu/2):\n",
    "        return (int(x)+1)/float(mu)\n",
    "    return int(x)/float(mu)\n",
    "ron = np.vectorize(rounder)\n",
    "\n",
    "def loss(y,yp):\n",
    "    return metrics.log_loss(y,ron(yp))\n",
    "\n",
    "def exo(cut,pca = 24,onTrain=0):\n",
    "    pca = PCA_transformed(pca)\n",
    "    X=pca.transform(cut[features])\n",
    "    X = np.concatenate([X,cut[[\"TSNE_1\",\"TSNE_2\"]]],axis=1)     # adding TSNE features\n",
    "    if onTrain==2:\n",
    "        return X\n",
    "    y=cut['target_'+target]\n",
    "    if onTrain:\n",
    "        return X,y\n",
    "    yp=cut['pred']\n",
    "    return X,y,yp,loss(y,yp)\n",
    "\n",
    "# Caluclates consistency score and log loss\n",
    "def eraLoss(frame,pred,pca):\n",
    "    \n",
    "    eras=frame['era'].unique()\n",
    "    frame['pred']=pred\n",
    "    li=[]\n",
    "    for era in eras:\n",
    "        X,y,yp,l=exo(frame[frame['era']==era], pca=pca)\n",
    "        # X = np.concatenate(X,tsne_features)\n",
    "        li.append(l)\n",
    "    li=np.array(li)\n",
    "    X,y,yp,l=exo(frame,pca)\n",
    "    c=len(li[li<.693])*100/len(li)\n",
    "    print ('Loss:', l, '- at ',c,'%')\n",
    "    print ('-- -- '*8)\n",
    "    print()\n",
    "    return l, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rr(x,y):\n",
    "    if type(x)==type(1) and type(y)==type(1):\n",
    "        x=int(x)\n",
    "        y=int(y)\n",
    "        return random.randint(x,y+1)\n",
    "    return random.uniform(x,y)\n",
    "\n",
    "def rrr(a,b): #test b4 use\n",
    "    if type(a) == int and type(b) == int:\n",
    "        c= int((b-a)/4)\n",
    "    else:\n",
    "        c= (b-a)/4\n",
    "    if random.choice([0,1]):\n",
    "        return rr(a,a+c)                   # search in first  quadrant\n",
    "    return rr(b-c,b)                       # search in fourth quadrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def param_gen(emap):                       # emap is a dictionary which has range of values [a,b] for each parameter\n",
    "    m={}\n",
    "    for i in emap:\n",
    "        m[i]=rrr(emap[i][0],emap[i][1])    # for each paramter in the dict provide the upper and lower limit values i.e a and b\n",
    "    return m                               # return a dictionary with single value for each parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mutate(a,b):\n",
    "    ones=[a,b]\n",
    "    if type(a) == int and type(b) == int:\n",
    "        muls=[int((a+b)/2),int((2*a+b)/3),int((2*b+a)/3)]\n",
    "    else:\n",
    "        muls=[(a+b)/2,(2*a+b)/3,(2*b+a)/3]\n",
    "#     print (muls)\n",
    "#     print (ones+muls+muls)\n",
    "    return random.choice(ones+muls+muls)\n",
    "\n",
    "def params_child(p1,p2):\n",
    "#     print (p1,p2)\n",
    "    m={}\n",
    "    for i in p1:                         \n",
    "        if p1[i]==p2[i]:\n",
    "            m[i]=p1[i]                          # When parameters are same for both the parents keep them in child else mutate\n",
    "        else:                                   \n",
    "            m[i]=mutate(p1[i],p2[i])\n",
    "    if m==p1:\n",
    "        return 0\n",
    "    if m==p2:\n",
    "        return 0\n",
    "    return m\n",
    "def rands(n):\n",
    "    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModel(ty='xgb',params={}):\n",
    "    if ty=='xgb':\n",
    "        copy = dict(params)                # making a copy of dict, to delete key safely\n",
    "        del copy[\"pca\"]\n",
    "        copy['nthread']=-1\n",
    "        return XGBClassifier(**copy)\n",
    "    if ty=='cb':\n",
    "        copy = dict(params)\n",
    "        del copy[\"pca\"]\n",
    "        copy['thread_count']= -1\n",
    "        copy['verbose']= False\n",
    "        return CatBoostClassifier(**copy)\n",
    "    if ty=='lgbm':\n",
    "        copy = dict(params)\n",
    "        del copy[\"pca\"]\n",
    "        return LGBMClassifier(**copy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maps={\n",
    "    'xgb':{'params':{'max_depth':[1,15],'n_estimators':[10,2000],'learning_rate':[.01,.9],'gamma':[0,100], 'min_child_weight':[1,20],\n",
    "           'max_delta_step':[1,1000], 'subsample':[.05,1.0],'colsample_bytree':[.05,1.0], \n",
    "           'colsample_bylevel':[.05,1.0], 'reg_alpha':[0,1000], 'reg_lambda':[0,1000], 'pca': [5,35]},\n",
    "           'hookups':4,\n",
    "           'children':3,\n",
    "           'asuras':2,\n",
    "           'survivors':5\n",
    "          },\n",
    "    'lgbm':{'params':{'num_leaves':[2,80],'max_depth':[1,20],'n_estimators':[10,2000],'learning_rate':[.01,.9],\n",
    "           'subsample':[.05,1.0],'colsample_bytree':[.05,1.0],'reg_alpha':[0,1000],'reg_lambda':[0,1000],'pca': [5,35]},\n",
    "           'hookups':10,\n",
    "           'children':4,\n",
    "           'asuras':6,\n",
    "           'survivors':9\n",
    "          },\n",
    "    'cb':{'params':{'depth':[1,15],\n",
    "          'iterations':[250,750],\n",
    "          'learning_rate':[0.03,0.3], \n",
    "          'l2_leaf_reg':[3,100],\n",
    "          'border_count':[32,200],'pca': [5,35]\n",
    "          },\n",
    "          'hookups':4,\n",
    "           'children':4,\n",
    "           'asuras':4,\n",
    "          'survivors':5\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this populates the performance of all the classifiers in genome dataframe\n",
    "def simulate():\n",
    "    for i,j in genome.iterrows():\n",
    "        if not j['ran']:\n",
    "            clock1=timeit.timeit()\n",
    "            print (j['name'])\n",
    "            clf=getModel(j['type'],j['params'])\n",
    "            X,y = exo(train,j['params'][\"pca\"],1)\n",
    "            Xt,yt=exo(ship,j['params'][\"pca\"],1)\n",
    "            clf.fit(X,y)\n",
    "            yp=clf.predict_proba(Xt)[:,1]\n",
    "            l,c=eraLoss(ship,yp,j['params'][\"pca\"])\n",
    "            clock2=timeit.timeit()\n",
    "            genome.loc[i,'log_loss']=l\n",
    "            genome.loc[i,'consistency']=c\n",
    "            genome.loc[i,'delay']=round((clock2-clock1)*1000,2)\n",
    "            genome.loc[i,'ran']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genesis():\n",
    "    for m in maps:\n",
    "        for i in range(maps[m]['asuras']):\n",
    "            genome.loc[len(genome)]=[m+'-gen0-orig-'+rands(4),m,param_gen(maps[m]['params']),.8,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reproduce(gen):\n",
    "    for m in maps:\n",
    "        for h in range(maps[m]['hookups']):\n",
    "            parents = genome[genome['type']==m].sample(n=2)\n",
    "            for c in range(maps[m]['children']):\n",
    "                child=params_child(parents.iloc[0]['params'],parents.iloc[1]['params'])\n",
    "                if child:\n",
    "                    genome.loc[len(genome)]=[m+'-gen'+str(gen)+'-h'+str(h)+'-c'+str(c)+'-'+rands(4),m,child,0.8,0,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def war(genome):\n",
    "    count=0\n",
    "    survivors=[]\n",
    "    for m in maps:\n",
    "        if count:\n",
    "            sids=genome[genome['type']==m].sort_values(['consistency','log_loss'], ascending=[False,True]).head(maps[m]['survivors']).index\n",
    "            survivors=survivors.append(sids)\n",
    "        else:\n",
    "            survivors=genome[genome['type']==m].sort_values(['consistency','log_loss'], ascending=[False,True]).head(maps[m]['survivors']).index\n",
    "        count=count+1\n",
    "    return genome.iloc[survivors].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genome=pd.DataFrame(columns=['name','type','params','log_loss','consistency','delay','ran'])\n",
    "\n",
    "genome=pd.read_csv('../genome.csv')\n",
    "genome['params']=genome['params'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submission_func(prediction_probabilities, path, target):\n",
    "    results_df = pd.DataFrame(data=prediction_probabilities,columns=['probability_'+target])\n",
    "    joined = pd.DataFrame(tour['id']).join(results_df)\n",
    "    assert(len(joined) == len(tour))\n",
    "    joined.to_csv(path, index=False)\n",
    "    \n",
    "    # Adding public and private keys\n",
    "    a='6X4PZS5QYKRW5RIN6LK4PY3KGAIQBZJPEW5CPEXNAM6IRBDTDLG4QV3NFWMMJQZI'\n",
    "    b='WWYHT5NBSLOPKL6VLPNFRZMRG5MHDKPI'\n",
    "    napi = numerapi.NumerAPI(b, a)\n",
    "    \n",
    "    tim={'bernie':1, 'charles':5, 'elizabeth':2, 'jordan':3, 'ken':4, 'frank':6, 'hillary':7}\n",
    "    \n",
    "    try:\n",
    "        submission_id = napi.upload_predictions(path,tournament=tim[target])\n",
    "        print (\"SUBMISSION UPLOADED for {}\".format(target))\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        print (\"\\n Error Uploading submission for {} \\n\".format(target))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in targets:\n",
    "    target=x\n",
    "    print (target)\n",
    "    \n",
    "    for generation in range(2,5):\n",
    "        reproduce(generation)\n",
    "        genesis()\n",
    "        simulate()\n",
    "        genome=war(genome)\n",
    "        genome.to_csv('../genome.csv',index=False)\n",
    "        genome.to_csv('../genome-gen'+str(generation)+'.csv',index=False)\n",
    "\n",
    "    # Stacking by kfold split\n",
    "\n",
    "\n",
    "    train_copy = train.copy()\n",
    "    tour_copy = tour.copy()\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import GroupKFold\n",
    "\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    X,Y = exo (train,1,onTrain=1)   # We just need index so PCA -1 does not matter here             \n",
    "    kfold_split = gkf.split(X, Y, groups=train.era)\n",
    "\n",
    "    for train_index, test_index in kfold_split:\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "        for i,j in genome.iterrows():\n",
    "            clf=getModel(j['type'],j['params'])\n",
    "            X,Y = exo(train,j['params'][\"pca\"],1)               # get X (train) as per n_components in PCA\n",
    "            Xt,yt=exo(tour,j['params'][\"pca\"],1)                # get X (tour) as per n_components in PCA\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = Y[train_index], Y[test_index]\n",
    "            clf.fit(X_train,y_train)\n",
    "            train_copy.loc[test_index,\"feature_{}_{}\".format(j['type'],i)]=clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "            clf.fit(X,Y)\n",
    "            tour_copy[\"feature_{}_{}\".format(j['type'],i)]=clf.predict_proba(Xt)[:,1]  \n",
    "\n",
    "\n",
    "    # Training a stacker\n",
    "    features_new=[f for f in list(train_copy) if \"feature_\" in f]\n",
    "    X_reg = train_copy[features_new]\n",
    "    y_reg = train_copy['target_'+target]\n",
    "\n",
    "    # Can add a different stacker here probably neural network!\n",
    "    stacker = LogisticRegression(max_iter=250).fit(X_reg,y_reg)\n",
    "    final_preds = stacker.predict_proba(tour_copy[features_new])[:,1]\n",
    "\n",
    "      \n",
    "    result_path ='../submissions/'+target+\"meta_model_stacked_submission.csv\"\n",
    "    \n",
    "    submission_func(final_preds,result_path,target)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0,1,2],[1,1,1]])\n",
    "b = np.array([[3,4],[2,2]])\n",
    "c = np.concatenate([a,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4],\n",
       "       [1, 1, 1, 2, 2]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.array([[1,2,3,4,5],[1,1,1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5],\n",
       "       [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_train = tsne_all[:X_train.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat with tsne features with train data\n",
    "X = np.concatenate([X, data_tsne[:train.shape[0]]], axis=1)\n",
    "\n",
    "# Concat with tsne features with validation data\n",
    "X = np.concatenate([X, data_tsne[:train.shape[0]]], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
